<h1 id="toc_0">General</h1>

<h2 id="toc_1">Is Stratosphere a Hadoop Project?</h2>

<p>Stratosphere is a data processing system and an alternative to Hadoop&#39;s
MapReduce component. It comes with its own runtime, rather than building on top
of MapReduce. As such, it can work completely independently of the Hadoop
ecosystem. However, Stratosphere can also access Hadoop&#39;s distributed file
system (HDFS) to read and write data, and Hadoop&#39;s next-generation resource
manager (YARN) to provision cluster resources. Since most Stratosphere users are
using Hadoop HDFS to store their data, we ship already the required libraries to
access HDFS.</p>

<h2 id="toc_2">Do I have to install Apache Hadoop to use Stratosphere?</h2>

<p>No. Stratosphere can run without a Hadoop installation. However, a very common
setup is to use Stratosphere to analyze data stored in the Hadoop Distributed
File System (HDFS). To make these setups work out of the box, we bundle the
Hadoop client libraries with Stratosphere by default.</p>

<p>Additionally, we provide a special YARN Enabled download of Stratosphere for
users with an existing Hadoop YARN cluster. <a href="http://hadoop.apache.org/docs/r2.2.0/hadoop-yarn/hadoop-yarn-%0Asite/YARN.html">Apache Hadoop
YARN</a> is Hadoop&#39;s cluster resource manager that allows to use
different execution engines next to each other on a cluster.</p>

<h1 id="toc_3">Usage</h1>

<h2 id="toc_4">How do I assess the progress of a Stratosphere program?</h2>

<p>There are a multiple of ways to track the progress of a Stratosphere program:</p>

<ul>
<li>The JobManager (the master of the distributed system) starts a web interface
to observe program execution. In runs on port 8081 by default (configured in
<code>conf/stratosphere-config.yml</code>).</li>
<li>When you start a program from the command line, it will print the status
changes of all operators as the program progresses through the operations.</li>
<li>All status changes are also logged to the JobManager&#39;s log file.</li>
</ul>

<h2 id="toc_5">How can I figure out why a program failed?</h2>

<ul>
<li>Thw JobManager web frontend (by default on port 8081) displays the exceptions
of failed tasks.</li>
<li>If you run the program from the command-line, task exceptions are printed to
the standard error stream and shown on the console.</li>
<li>Both the command line and the web interface allow you to figure out which
parallel task first failed and caused the other tasks to cancel the execution.</li>
<li>Failing tasks and the corresponding exceptions are reported in the log files
of the master and the worker where the exception occurred
(<code>log/stratosphere-&lt;user&gt;-jobmanager-&lt;host&gt;.log</code> and
<code>log/stratosphere-&lt;user&gt;-taskmanager-&lt;host&gt;.log</code>).</li>
</ul>

<h2 id="toc_6">How do I debug Stratosphere programs?</h2>

<ul>
<li>When you start a program locally with the <a href="local_execution.html">LocalExecutor</a>,
you can place breakpoints in your functions and debug them like normal
Java/Scala programs.</li>
<li>The <a href="java_api_guide.html#accumulators">Accumulators</a> are very helpful in
tracking the behavior of the parallel execution. They allow you to gather
information inside the program&#39;s operations and show them after the program
execution.</li>
</ul>

<h1 id="toc_7">Errors</h1>

<h2 id="toc_8">I get an error message saying that not enough buffers are available. How do I fix this?</h2>

<p>If you run Stratosphere in a massively parallel setting (100+ parallel threads),
you need to adapt the number of network buffers via the config parameter
<code>taskmanager.network.numberOfBuffers</code>.
As a rule-of-thumb, the number of buffers should be at least
<code>4 * numberOfNodes * numberOfTasksPerNode^2</code>. See
<a href="config.html">Configuration Reference</a> for details.</p>

<h2 id="toc_9">My job fails early with a java.io.EOFException. What could be the cause?</h2>

<p>Note: In version <em>0.4</em>, the delta iterations limit the solution set to
records with fixed-length data types. We will  in the next version.</p>

<p>The most common case for these exception is when Stratosphere is set up with the
wrong HDFS version. Because different HDFS versions are often not compatible
with each other, the connection between the filesystem master and the client
breaks.</p>
<div class="highlight"><pre><code class="bash language-bash" data-lang="bash">Call to &lt;host:port&gt; failed on <span class="nb">local </span>exception: java.io.EOFException
    at org.apache.hadoop.ipc.Client.wrapException<span class="o">(</span>Client.java:775<span class="o">)</span>
    at org.apache.hadoop.ipc.Client.call<span class="o">(</span>Client.java:743<span class="o">)</span>
    at org.apache.hadoop.ipc.RPC<span class="nv">$Invoker</span>.invoke<span class="o">(</span>RPC.java:220<span class="o">)</span>
    at <span class="nv">$Proxy0</span>.getProtocolVersion<span class="o">(</span>Unknown Source<span class="o">)</span>
    at org.apache.hadoop.ipc.RPC.getProxy<span class="o">(</span>RPC.java:359<span class="o">)</span>
    at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode<span class="o">(</span>DFSClient.java:106<span class="o">)</span>
    at org.apache.hadoop.hdfs.DFSClient.&lt;init&gt;<span class="o">(</span>DFSClient.java:207<span class="o">)</span>
    at org.apache.hadoop.hdfs.DFSClient.&lt;init&gt;<span class="o">(</span>DFSClient.java:170<span class="o">)</span>
    at org.apache.hadoop.hdfs.DistributedFileSystem.initialize<span class="o">(</span>DistributedFileSystem.java:82<span class="o">)</span>
    at eu.stratosphere.runtime.fs.hdfs.DistributedFileSystem.initialize<span class="o">(</span>DistributedFileSystem.java:276
</code></pre></div>
<p>Please refer to the <a href="http://stratosphere.eu/downloads/#maven">download page</a> and
the <a href="https://github.com/stratosphere/stratosphere/blob/master/README.md">build instructions</a>
for details on how to set up Stratosphere for different Hadoop and HDFS versions.</p>

<h2 id="toc_10">My program does not compute the correct result. Why are my custom key types</h2>

<p>are not grouped/joined correctly?</p>

<p>Keys must correctly implement the methods <code>java.lang.Object#hashCode()</code>,
<code>java.lang.Object#equals(Object o)</code>, and <code>java.util.Comparable#compareTo(...)</code>.
These methods are always backed with default implementations which are usually
inadequate. Therefore, all keys must override <code>hashCode()</code> and <code>equals(Object o)</code>.</p>

<h2 id="toc_11">I get a java.lang.InstantiationException for my data type, what is wrong?</h2>

<p>All data type classes must be public and have a public nullary constructor
(constructor with no arguments). Further more, the classes must not be abstract
or interfaces. If the classes are internal classes, they must be public and
static.</p>

<h2 id="toc_12">I can&#39;t stop Stratosphere with the provided stop-scripts. What can I do?</h2>

<p>Stopping the processes sometimes takes a few seconds, because the shutdown may
do some cleanup work.</p>

<p>In some error cases it happens that the JobManager or TaskManager cannot be
stopped with the provided stop-scripts (<code>bin/stop-local.sh</code> or <code>bin/stop-
cluster.sh</code>). You can kill their processes on Linux/Mac as follows:</p>

<ul>
<li>Determine the process id (pid) of the JobManager / TaskManager process. You
can use the <code>jps</code> command on Linux(if you have OpenJDK installed) or command
<code>ps -ef | grep java</code> to find all Java processes. </li>
<li>Kill the process with <code>kill -9 &lt;pid&gt;</code>, where <code>pid</code> is the process id of the
affected JobManager or TaskManager process.</li>
</ul>

<p>On Windows, the TaskManager shows a table of all processes and allows you to
destroy a process by right its entry.</p>

<h2 id="toc_13">I got an OutOfMemoryException. What can I do?</h2>

<p>These exceptions occur usually when the functions in the program consume a lot
of memory by collection large numbers of objects, for example in lists or maps.
The OutOfMemoryExceptions in Java are kind of tricky. The exception is not
necessarily thrown by the component that allocated most of the memory but by the
component that tried to requested the latest bit of memory that could not be
provided.</p>

<p>There are two ways to go about this:</p>

<ol>
<li><p>See whether you can use less memory inside the functions. For example, use
arrays of primitive types instead of object types.</p></li>
<li><p>Reduce the memory that Stratosphere reserves for its own processing. The
TaskManager reserves a certain portion of the available memory for sorting,
hashing, caching, network buffering, etc. That part of the memory is unavailable
to the user-defined functions. By reserving it, the system can guarantee to not
run out of memory on large inputs, but to plan with the available memory and
destage operations to disk, if necessary. By default, the system reserves around
70% of the memory. If you frequently run applications that need more memory in
the user-defined functions, you can reduce that value using the configuration
entries <code>taskmanager.memory.fraction</code> or <code>taskmanager.memory.size</code>. See the
<a href="http://stratosphere.eu/docs/0.4/setup/config.html" title="Configuration Reference">Configuration Reference</a> for details. This will leave more memory to JVM heap,
but may cause data processing tasks to go to disk more often.</p></li>
</ol>

<h2 id="toc_14">Why do the TaskManager log files become so huge?</h2>

<p>Check the logging behavior of your jobs. Emitting logging per or tuple may be
helpful to debug jobs in small setups with tiny data sets, it becomes very
inefficient and disk space consuming if used for large input data.</p>

<h1 id="toc_15">YARN Deployment</h1>

<h2 id="toc_16">The YARN session runs only for a few seconds</h2>

<p>The <code>./bin/yarn-session.sh</code> script is intended to run while the YARN-session is
open. In some error cases however, the script immediately stops running. The
output looks like this:</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">07:34:27,004 INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl         - Submitted application application_1395604279745_273123 to ResourceManager at jobtracker-host
Stratosphere JobManager is now running on worker1:6123
JobManager Web Interface: http://jobtracker-host:54311/proxy/application_1295604279745_273123/
07:34:51,528 INFO  eu.stratosphere.yarn.Client                                   - Application application_1295604279745_273123 finished with state FINISHED at 1398152089553
07:34:51,529 INFO  eu.stratosphere.yarn.Client                                   - Killing the Stratosphere-YARN application.
07:34:51,529 INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl         - Killing application application_1295604279745_273123
07:34:51,534 INFO  eu.stratosphere.yarn.Client                                   - Deleting files in hdfs://user/marcus/.stratosphere/application_1295604279745_273123
07:34:51,559 INFO  eu.stratosphere.yarn.Client                                   - YARN Client is shutting down
</code></pre></div>
<p>The problem here is that the Application Master (AM) is stopping and the YARN client assumes that the application has finished.</p>

<p>There are three possible reasons for that behavior:</p>

<ul>
<li><p>The ApplicationMaster exited with an exception. To debug that error, have a
look in the logfiles of the container. The <code>yarn-site.xml</code> file contains the
configured path. The key for the path is <code>yarn.nodemanager.log-dirs</code>, the
default value is <code>${yarn.log.dir}/userlogs</code>.</p></li>
<li><p>YARN has killed the container that runs the ApplicationMaster. This case
happens when the AM used too much memory or other resources beyond YARN&#39;s
limits. In this case, you&#39;ll find error messages in the nodemanager logs on
the host.</p></li>
<li><p>The operating system has shut down the JVM of the AM. This can happen if the
YARN configuration is wrong and more memory than physically available is
configured. Execute <code>dmesg</code> on the machine where the AM was running to see if
this happened. You see messages from Linux&#39; <a href="http://linux-mm.org/OOM_Killer">OOM killer</a>.</p></li>
</ul>

<h2 id="toc_17">The YARN session crashes with a HDFS permission exception during startup</h2>

<p>While starting the YARN session, you are receiving an exception like this:</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">Exception in thread &quot;main&quot; org.apache.hadoop.security.AccessControlException: Permission denied: user=robert, access=WRITE, inode=&quot;/user/robert&quot;:hdfs:supergroup:drwxr-xr-x
  at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:234)
  at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:214)
  at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:158)
  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:5193)
  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:5175)
  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAncestorAccess(FSNamesystem.java:5149)
  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2090)
  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2043)
  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1996)
  at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:491)
  at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:301)
  at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java:59570)
  at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
  at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
  at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2053)
  at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
  at java.security.AccessController.doPrivileged(Native Method)
  at javax.security.auth.Subject.doAs(Subject.java:396)
  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)
  at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2047)

  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
  at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
  at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
  at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
  at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1393)
  at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1382)
  at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1307)
  at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:384)
  at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:380)
  at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
  at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:380)
  at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:324)
  at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:905)
  at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:886)
  at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:783)
  at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:365)
  at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:338)
  at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:2021)
  at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:1989)
  at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:1954)
  at eu.stratosphere.yarn.Utils.setupLocalResource(Utils.java:176)
  at eu.stratosphere.yarn.Client.run(Client.java:362)
  at eu.stratosphere.yarn.Client.main(Client.java:568)
</code></pre></div>
<p>The reason for this error is, that the home directory of the user <strong>in HDFS</strong>
has the wrong permissions. The user (in this case <code>robert</code>) can not create
directories in his own home directory.</p>

<p>Stratosphere creates a <code>.stratosphere/</code> directory in the users home directory
where it stores the Stratosphere jar and configuration file.</p>

<h1 id="toc_18">Features</h1>

<h2 id="toc_19">What kind of fault-tolerance does Stratosphere provide?</h2>

<p>Stratospere can restart failed jobs. Mid-query fault tolerance will go into the
open source project in the next versions.</p>

<h2 id="toc_20">Are Hadoop-like utilities, such as Counters and the DistributedCache supported?</h2>

<p><a href="java_api_guide.html">Stratosphere&#39;s Accumulators</a> work very similar like
[Hadoop&#39;s counters, but are more powerful.</p>

<p>Stratosphere has a <a href="https://github.com/stratosphere/stratosphere/blob//stratosphere-core/src/main/java/eu/stratosphere/api/common/cache/DistributedCache.java">Distributed Cache</a> that is deeply integrated with the APIs. Please refer to the <a href="https://github.com/stratosphere/stratosphere/blob//stratosphere-java/src/main/java/eu/stratosphere/api/java/ExecutionEnvironment.java#L561">JavaDocs</a> for details on how to use it.</p>

<p>In order to make data sets available on all tasks, we encourage you to use <a href="/incubator-flink-website/docs/0.5/programming_guides/java.html#broadcast_variables">Broadcast Variables</a> instead. They are more efficient and easier to use than the distributed cache.</p>
